{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Metadata Extraction & Fact Verification Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow for extracting benchmark metadata, identifying risks, and performing fact-based verification using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **UnitXT Lookup** - Extract benchmark metadata from catalog\n",
    "2. **ID Extraction** - Parse HuggingFace repo IDs and paper URLs\n",
    "3. **HuggingFace Metadata** - Fetch dataset information\n",
    "4. **Paper Extraction** - Extract content from academic papers using Docling\n",
    "5. **Card Composition** - Generate structured benchmark cards using LLM\n",
    "6. **Risk Identification** - Identify potential risks using AI Atlas Nexus\n",
    "7. **RAG Processing** - Retrieve evidence for fact verification\n",
    "8. **Factuality Evaluation** - Assess confidence and flag uncertain fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "Install dependencies. FactReasoner and AI Atlas Nexus are now installed from pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-17 13:19:15:840] - INFO - RiskAtlasNexus - Created RITS inference engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import workflow components\n",
    "from auto_benchmarkcard.workflow import build_workflow, OutputManager, sanitize_benchmark_name\n",
    "from auto_benchmarkcard.config import Config\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your API credentials and processing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured\n",
      "LLM Engine: rits\n",
      "Model: llama-3.3-70b-instruct\n",
      "Threshold: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify credentials\n",
    "required = ['RITS_API_KEY', 'RITS_MODEL', 'RITS_API_URL']\n",
    "missing = [v for v in required if not os.getenv(v)]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"Environment configured\")\n",
    "\n",
    "# Show config\n",
    "print(f\"LLM Engine: {Config.LLM_ENGINE_TYPE}\")\n",
    "print(f\"Model: {Config.DEFAULT_MODEL}\")\n",
    "print(f\"Threshold: {Config.DEFAULT_FACTUALITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Workflow\n",
    "\n",
    "Execute the complete pipeline for a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing benchmark: glue\n"
     ]
    }
   ],
   "source": [
    "# Define benchmark to process\n",
    "BENCHMARK_QUERY = \"glue\"  # Change this to your desired benchmark\n",
    "CATALOG_PATH = None  # Optional: custom UnitXT catalog path\n",
    "OUTPUT_PATH = None  # Optional: custom output directory\n",
    "\n",
    "print(f\"Processing benchmark: {BENCHMARK_QUERY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session: output/glue_2025-10-17_13-19\n",
      "=== Starting Workflow ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:19:18,921 - INFO - UnitXT metadata retrieved\n",
      "2025-10-17 13:19:18,923 - INFO - UnitXT output saved to: output/glue_2025-10-17_13-19/tool_output/unitxt/glue.json\n",
      "2025-10-17 13:19:18,924 - INFO - Starting ID and URL extraction\n",
      "2025-10-17 13:19:18,924 - INFO - ID extraction completed\n",
      "2025-10-17 13:19:18,925 - INFO - Extracted: HF=['nyu-mll/glue', 'stanfordnlp/sst2'], Paper=None\n",
      "2025-10-17 13:19:18,926 - INFO - Extractor output saved to: output/glue_2025-10-17_13-19/tool_output/extractor/glue.json\n",
      "2025-10-17 13:19:20,060 - INFO - HuggingFace metadata retrieved successfully\n",
      "2025-10-17 13:19:20,064 - INFO - HuggingFace output saved to: output/glue_2025-10-17_13-19/tool_output/hf/glue.json\n",
      "2025-10-17 13:19:20,066 - INFO - Starting HuggingFace extraction\n",
      "2025-10-17 13:19:20,067 - INFO - Found paper_url in HF dataset nyu-mll/glue: https://arxiv.org/abs/1804.07461\n",
      "2025-10-17 13:19:20,069 - INFO - HF extractor output saved to: output/glue_2025-10-17_13-19/tool_output/extractor/glue.json\n",
      "2025-10-17 13:19:20,070 - INFO - Starting paper extraction\n",
      "2025-10-17 13:19:20,071 - INFO - Extracting paper from: https://arxiv.org/abs/1804.07461\n",
      "2025-10-17 13:19:52,817 - INFO - Docling extraction completed successfully\n",
      "2025-10-17 13:19:52,818 - INFO - Paper: GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING (109,325 chars)\n",
      "2025-10-17 13:19:52,819 - INFO - Docling output saved to: output/glue_2025-10-17_13-19/tool_output/docling/glue.json\n",
      "2025-10-17 13:19:52,820 - INFO - Starting benchmark card composition\n",
      "2025-10-17 13:21:35,873 - INFO - Successfully composed benchmark card\n",
      "2025-10-17 13:21:35,874 - INFO - Card: GLUE | natural language understanding, sentence classification | English\n",
      "2025-10-17 13:21:35,876 - INFO - Starting risk identification\n",
      "[2025-10-17 13:21:36:280] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "[2025-10-17 13:21:36:631] - INFO - RiskAtlasNexus - Created RiskAtlasNexus instance. Base_dir: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ea4c0478de4524899a5c99b95f749e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferring with RITS:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 13:21:39,731 - INFO - Risk-enhanced card saved to: output/glue_2025-10-17_13-19/tool_output/risk_enhanced/glue.json\n",
      "2025-10-17 13:21:39,732 - INFO - Risk identification results saved to: output/glue_2025-10-17_13-19/tool_output/risk_atlas_nexus/risks_glue.json\n",
      "2025-10-17 13:21:39,733 - INFO - Risk identification completed\n",
      "2025-10-17 13:21:39,733 - INFO - Risks: Membership inference attack, Confidential data in prompt (+3 more)\n",
      "2025-10-17 13:21:39,733 - INFO - Starting RAG processing\n",
      "[2025-10-17 13:23:43:539] - INFO - RiskAtlasNexus - Created RITS inference engine.\n",
      "2025-10-17 13:24:29,789 - INFO - RAG processing completed\n",
      "2025-10-17 13:24:29,789 - INFO - RAG: 18 claims, 47 evidence sources\n",
      "2025-10-17 13:24:29,790 - INFO - RAG results saved to: output/glue_2025-10-17_13-19/tool_output/rag/formatted_rag_results_glue.jsonl\n",
      "2025-10-17 13:24:29,791 - INFO - Starting factuality evaluation\n",
      "2025-10-17 13:24:31,553 - INFO - FactReasoner evaluation complete\n",
      "2025-10-17 13:24:31,554 - INFO - Factuality: 18 claims evaluated, 9/18 fields flagged\n",
      "2025-10-17 13:24:31,555 - INFO - Factuality results saved to: output/glue_2025-10-17_13-19/tool_output/factreasoner/factuality_results_glue.json\n",
      "2025-10-17 13:24:31,555 - INFO - Final benchmark card saved to: output/glue_2025-10-17_13-19/benchmarkcard/benchmark_card_glue.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize output manager\n",
    "output_manager = OutputManager(BENCHMARK_QUERY, OUTPUT_PATH)\n",
    "print(f\"Session: {output_manager.get_summary()['session_directory']}\")\n",
    "\n",
    "# Initialize workflow state (all keys required by workflow)\n",
    "initial_state = {\n",
    "    \"query\": BENCHMARK_QUERY,\n",
    "    \"catalog_path\": CATALOG_PATH,\n",
    "    \"output_manager\": output_manager,\n",
    "    \"unitxt_json\": None,\n",
    "    \"extracted_ids\": None,\n",
    "    \"hf_repo\": None,\n",
    "    \"hf_json\": None,\n",
    "    \"docling_output\": None,\n",
    "    \"composed_card\": None,\n",
    "    \"risk_enhanced_card\": None,\n",
    "    \"completed\": [],\n",
    "    \"errors\": [],\n",
    "    \"hf_extraction_attempted\": False,\n",
    "    \"rag_results\": None,\n",
    "    \"factuality_results\": None,\n",
    "}\n",
    "\n",
    "# Build and execute workflow\n",
    "print(\"=== Starting Workflow ===\")\n",
    "workflow = build_workflow()\n",
    "\n",
    "try:\n",
    "    final_state = workflow.invoke(initial_state)\n",
    "    print(\"Workflow completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Workflow failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Workflow Steps Completed ===\n",
      "1. unitxt done\n",
      "2. extract hf_repo=['nyu-mll/glue', 'stanfordnlp/sst2'], paper_url=None\n",
      "3. hf done\n",
      "4. hf_extract paper_url=https://arxiv.org/abs/1804.07461\n",
      "5. docling done\n",
      "6. composer done\n",
      "7. risk identification done\n",
      "8. rag done\n",
      "9. factreasoner done\n"
     ]
    }
   ],
   "source": [
    "# Display workflow steps\n",
    "print(\"=== Workflow Steps Completed ===\")\n",
    "for i, step in enumerate(final_state.get('completed', []), 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "# Display errors if any\n",
    "errors = final_state.get('errors', [])\n",
    "if errors:\n",
    "    print(\"=== Errors ===\")\n",
    "    for error in errors:\n",
    "        print(f\"{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Benchmark Card\n",
    "\n",
    "Display the composed benchmark card with risk information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Benchmark Details ===\n",
      "Name: GLUE\n",
      "Domains: ['natural language understanding', 'sentence classification', 'textual entailment']\n",
      "Languages: ['English']\n",
      "\n",
      "Risks Identified: 5\n",
      "=== Full Benchmark Card ===\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "benchmark_details": {
        "data_type": "text",
        "domains": [
         "natural language understanding",
         "sentence classification",
         "textual entailment"
        ],
        "languages": [
         "English"
        ],
        "name": "GLUE",
        "overview": "The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems.",
        "resources": [
         "https://gluebenchmark.com/",
         "https://arxiv.org/abs/1804.07461"
        ],
        "similar_benchmarks": [
         "SuperGLUE",
         "XTREME",
         "BigBench"
        ]
       },
       "card_info": {
        "created_at": "2025-10-17T13:24:31.553171",
        "llm": "llama-3.3-70b-instruct"
       },
       "data": {
        "annotation": "Professional annotators and crowdsourcing with quality control, inter-annotator agreement validation, and expert review",
        "format": "JSON format with task-specific fields including sentence pairs, labels, and metadata",
        "size": "108,000 total examples across 9 tasks with train/dev/test splits varying by task",
        "source": "Collected from GLUE benchmark"
       },
       "ethical_and_legal_considerations": {
        "compliance_with_regulations": "Not specified",
        "consent_procedures": "Not specified",
        "data_licensing": "Not specified",
        "privacy_and_anonymity": "Not specified"
       },
       "flagged_fields": {
        "benchmark_details.resources": "[Possible Hallucination], no supporting evidence found in source material",
        "benchmark_details.similar_benchmarks": "[Possible Hallucination], no supporting evidence found in source material",
        "data.annotation": "[Possible Hallucination], no supporting evidence found in source material",
        "data.size": "[Possible Hallucination], no supporting evidence found in source material",
        "methodology.baseline_results": "[Possible Hallucination], no supporting evidence found in source material",
        "methodology.interpretation": "[Factuality Score: 0.09], low factual alignment with source material",
        "methodology.methods": "[Possible Hallucination], no supporting evidence found in source material",
        "methodology.validation": "[Possible Hallucination], no supporting evidence found in source material",
        "purpose_and_intended_users.audience": "[Possible Hallucination], no supporting evidence found in source material"
       },
       "methodology": {
        "baseline_results": "GLUE benchmark results: https://gluebenchmark.com/leaderboard",
        "calculation": "Accuracy computed as correct predictions divided by total predictions, F1-score as harmonic mean of precision and recall, with macro-averaging across classes",
        "interpretation": "Scores range from 0-100, with higher scores indicating better performance. Baseline human performance is approximately Not specified",
        "methods": [
         "fine-tuning on task-specific data",
         "zero-shot evaluation",
         "few-shot learning"
        ],
        "metrics": [
         "accuracy",
         "F1-score",
         "Matthews correlation coefficient"
        ],
        "validation": "Cross-validation on development sets, held-out test sets, statistical significance testing, and comparison with human performance"
       },
       "missing_fields": [
        "ethical_and_legal_considerations.privacy_and_anonymity",
        "ethical_and_legal_considerations.data_licensing",
        "ethical_and_legal_considerations.consent_procedures",
        "ethical_and_legal_considerations.compliance_with_regulations"
       ],
       "possible_risks": [
        {
         "category": "Membership inference attack",
         "concern": "Identifying whether a data sample was used for training data can reveal what data was used to train a model, possibly giving competitors insight into how a model was trained and the opportunity to replicate the model or tamper with it. Models that include publicly-available data are at higher risk of such attacks.",
         "description": [
          "A membership inference attack repeatedly queries a model to determine if a given input was part of the model’s training. More specifically, given a trained model and a data sample, an attacker appropriately samples the input space, observing outputs to deduce whether that sample was part of the model’s training."
         ],
         "taxonomy": "ibm-risk-atlas",
         "type": "inference",
         "url": "https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/membership-inference-attack.html"
        },
        {
         "category": "Confidential data in prompt",
         "concern": "If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output. Additionally, end users' confidential information might be unintentionally collected and stored.",
         "description": [
          "Confidential information might be included as a part of the prompt that is sent to the model."
         ],
         "taxonomy": "ibm-risk-atlas",
         "type": "inference",
         "url": "https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/confidential-data-in-prompt.html"
        },
        {
         "category": "Prompt leaking",
         "concern": "A successful prompt leaking attack copies the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to replicate some of the functionality of the model.",
         "description": [
          "A prompt leak attack attempts to extract a model’s system prompt (also known as the system message)."
         ],
         "taxonomy": "ibm-risk-atlas",
         "type": "inference",
         "url": "https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/prompt-leaking.html"
        },
        {
         "category": "Harmful output",
         "concern": "A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm.",
         "description": [
          "A model might generate language that leads to physical harm. The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements."
         ],
         "taxonomy": "ibm-risk-atlas",
         "type": "output",
         "url": "https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/harmful-output.html"
        },
        {
         "category": "Data contamination",
         "concern": "Data that differs from the intended training data might skew model accuracy and affect model outcomes.",
         "description": [
          "Data contamination occurs when incorrect data is used for training. For example, data that is not aligned with model’s purpose or data that is already set aside for other development tasks such as testing and evaluation."
         ],
         "taxonomy": "ibm-risk-atlas",
         "type": "training-data",
         "url": "https://www.ibm.com/docs/en/watsonx/saas?topic=SSYOK8/wsj/ai-risk-atlas/data-contamination.html"
        }
       ],
       "purpose_and_intended_users": {
        "audience": [
         "NLP researchers",
         "machine learning engineers",
         "academic institutions",
         "AI practitioners"
        ],
        "goal": "The primary goal of the GLUE benchmark is to evaluate and analyze the performance of natural language understanding (NLU) systems across a diverse set of tasks, with the aim of developing more general and robust models.",
        "limitations": "The GLUE benchmark has several limitations, including the limited size of some datasets and the potential for biased models. Additionally, the benchmark is focused on English language tasks and may not generalize well to other languages.",
        "out_of_scope_uses": [
         "Using the GLUE benchmark as the sole evaluation metric for NLU systems",
         "Applying the GLUE benchmark to tasks outside of its intended scope, such as machine translation or speech recognition"
        ],
        "tasks": [
         "question answering",
         "sentiment analysis",
         "textual entailment",
         "linguistic acceptability"
        ]
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the final saved benchmark card from disk\n",
    "card_name = f\"benchmark_card_{sanitize_benchmark_name(BENCHMARK_QUERY)}.json\"\n",
    "card_path = Path(output_manager.benchmarkcard_dir) / card_name\n",
    "\n",
    "if card_path.exists():\n",
    "    with open(card_path, 'r') as f:\n",
    "        saved_card = json.load(f)\n",
    "\n",
    "    benchmark_card = saved_card.get('benchmark_card', {})\n",
    "    details = benchmark_card.get('benchmark_details', {})\n",
    "    risks = benchmark_card.get('possible_risks', [])\n",
    "\n",
    "    print(\"=== Benchmark Details ===\")\n",
    "    print(f\"Name: {details.get('name', 'N/A')}\")\n",
    "    print(f\"Domains: {details.get('domains', [])}\")\n",
    "    print(f\"Languages: {details.get('languages', [])}\")\n",
    "    print(f\"\\nRisks Identified: {len(risks)}\")\n",
    "\n",
    "    # Display full card\n",
    "    print(\"=== Full Benchmark Card ===\")\n",
    "    display(JSON(benchmark_card, expanded=False))\n",
    "else:\n",
    "    print(f\"No benchmark card found at: {card_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Outputs\n",
    "\n",
    "All results are saved in the session directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output Files ===\\n\n",
      "Session Directory: output/glue_2025-10-17_13-19\n",
      "Benchmark Cards: output/glue_2025-10-17_13-19/benchmarkcard\n",
      "Final card: output/glue_2025-10-17_13-19/benchmarkcard/benchmark_card_glue.json\n",
      "  Size: 8,312 bytes\n"
     ]
    }
   ],
   "source": [
    "# Display output locations\n",
    "summary = output_manager.get_summary()\n",
    "print(\"=== Output Files ===\\\\n\")\n",
    "print(f\"Session Directory: {summary['session_directory']}\")\n",
    "print(f\"Benchmark Cards: {summary['benchmark_cards']}\")\n",
    "\n",
    "# Final card path\n",
    "card_name = f\"benchmark_card_{sanitize_benchmark_name(BENCHMARK_QUERY)}.json\"\n",
    "card_path = Path(summary['benchmark_cards']) / card_name\n",
    "\n",
    "if card_path.exists():\n",
    "    print(f\"Final card: {card_path}\")\n",
    "    print(f\"  Size: {card_path.stat().st_size:,} bytes\")\n",
    "else:\n",
    "    print(f\"Card not found at: {card_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
